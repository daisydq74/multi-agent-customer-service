Through this project, I learned how to design a practical multi-agent workflow where an orchestrator (Router Agent) decomposes user requests, delegates sub-tasks to specialist agents, and then synthesizes a single customer-facing response. Implementing the MCP database tools made the system feel “real” rather than a toy demo: I had to think carefully about tool interfaces, data validation, and predictable outputs so agents could reliably collaborate. Migrating to an A2A-style message passing setup also helped me understand what it takes to make agent coordination inspectable and testable—clear agent roles, structured payloads, and logging that shows control transfer at each step.

The biggest challenges were debugging multi-step interactions and making the behavior robust across different query types. Small issues—like intent parsing edge cases, missing customer identifiers, or inconsistent data assumptions—could cause the system to take the wrong route or return incomplete logs. Another challenge was keeping the demo reliable: ensuring services start cleanly, the database is seeded/reset consistently, and outputs are reproducible for grading. Overall, the project highlighted that multi-agent systems succeed not just from “having agents,” but from disciplined interfaces, explicit coordination signals, and strong observability (logs + error handling) to prevent silent failures.
